

###  基础

#### 自动装箱与自动拆箱

- 在java中所有的基本类型都有对应的包装类，java提供了自动装箱与自动拆箱来简化开发，即可以直接将基本类型直接赋值给对应的包装类，也可以把对应的包装类直接赋值给对应的基本类型，自动装箱的原理是调用了包装类的valueOf方法，而自动拆箱就是调用了intValue这些方法；并且Integer类型在调用valueOf方法的时候会对-128到127这个范围内的整形进行缓存，即valueOf方法调用时，-128到127返回的是同一个Integer对象

#### 静态内部类与非静态内部类

- 内部类的加载是在类被使用到了的时候，与外部类的加载无关，即使外部类加载了，内部类也不一定会加载，只有使用到了的时候才会进行加载，只有静态内部类可以有静态成员变量和静态方法，非静态内部类不可以有静态成员与静态方法，可以有static常量；静态内部类只能访问到外部类的静态成员和静态方法，而非静态内部类可以访问到外部类的所有成员；静态内部的实例不依赖于外部类对象，即可以通过new外部类名点静态内部类名来创建一个静态内部类对象，而普通内部类既然能访问到外部类的所有属性与方法，那么自然需要先实例化外部类，普通内部类可以通过外部对象点new内部类名进行实例化

#### 接口与抽象类

- 接口使用interface关键字声明，抽象类使用abstract class声明，接口可以多继承接口，在实现一个类的时候，可以通过implement关键字去实现接口，并且支持多实现，用‘，’隔开，通过extends关键字继承抽象类，只能继承一个；在实现接口的时候，必须重写接口中的所有抽象方法，在继承抽象类的时候，如果当前类不是抽象类，则必须重写所有抽象类中的抽象方法，如果当前类是抽象类，那么可以不重写，但是非抽象子类要重写接口中的方法；接口和抽象类都不可以实例化，即使抽象类中没有抽象方法也不可以实例化；

###  集合

#### HashMap

- HashMap是一个以键值对存储元素的散列表，使用链地址法解决了hash冲突，在1.7中使用了数组加链表，在1.8中添加红黑树防止某个节点上的元素数量太多而导致查询的时间复杂度退化为o[n]；hashmap允许值为null，但只允许存入一个键为null的元素；

#### Arrays

- Arrays包含了许多用于操作数组的方法，比如Arrays的sort方法、copyOf方法、binarySerach方法；Arrays的sort方法根据数组中元素数量选择合适的排序算法，长度小于47使用插入排序，大于286时会根据数组的是否比较有序来选择双轴快排或者归并排序，而在47到286之间就是使用双轴快排

#### CopyOnWriteArrayList

- CopyOnWriteArrayList维护了一个ReentrantLock，在每次进行写操作的时候需要先获取锁才能进行操作，并且每次写操作会先将原数组通过System.arrayCopy复制到一个长度加一的数组中 ，再将新的数据添加到新数组中，然后将这个新数组替换原数组

#### HashSet与TreeSet

- HashSet和TreeSet的底层实现分别是HashMap和TreeMap，拿HashSet举例，它的api基本上是对HashMap的api的调用，不同的是，它会在map的每个元素的value处存放一个final类型的Object对象，即只使用key存值

#### ArrayList和LinkedList的不同点

- ArrayList的底层是一个动态数组，而LinkedList的底层是双向链表；所以ArrayList和LinkedList的不同点其实就差不多就是数组与链表的区别，所以ArrayList的随机访问的时间复杂度为O(1)，而LinkedList在访问数据的时候肯定需要遍历链表；以及由于ArrayList是有固定长度的，当元素满了之后，会扩容为原来的1.5倍

### 多线程

#### 线程安全的集合类

- 线程安全的集合类主要有Vector、CopyOnWriteArrayList、CopyOnWriteArraySet、ConcurrrentHashMap、HashTable、BlockingQueue、ConcurrentSkipListMap

#### 死锁产生条件及解决方式

- 当一组线程内的每个线程都在等待该组线程中其它线程才能触发的操作，那么这一组线程就是死锁的；死锁产生的条件有互斥条件、请求保持条件、不可剥夺条件和循环等待条件，其中互斥条件是多线程同步中本就该保护的条件，所以想要解决死锁，可以从其它三个条件入手，并且解决死锁本身是可以分为产生死锁前预防和避免和产生死锁后检测与解除的；先讲一下预防，从请求等待条件入手，原因是因为当线程去请求其它资源的时候，还不会放弃已经有的资源，所以解决方案就是当线程去获取资源的时候，一次性获取所有资源或者在请求其它资源前放弃已经使用完的资源；从不可剥夺条件入手，当一个线程去申请其它不可抢占资源的时候，要先释放已经获取的不可抢占资源，等需要时再获取；从循环等待入手，可以将所有资源从低到高进行排序，想获得低序列的资源时必须释放高序号的资源；避免通过提前判断的方式避免进入可能导致死锁的状态，比如银行安全家算法；检测和预防就是要记录好各种资源的状态和各线程持有资源的状态，再通过程序判断是否产生了死锁，产生了死锁后根据解除算法进行死锁解除

#### 设计一个线程安全的类

- 三个步骤：找出构成对象的所有状态、找出约束状态变量的不变性条件、建立对象状态的并发访问策略；就比如有一个集合HashMap和在HashMap基础上进行修改的并发安全的ConcurrentHashMap，就讲几个比较重要的成员变量吧，用于存放数据的entry数组，扩容阈值，元素数量；

#### CAS

- CAS是通过操作系统的原语来达到原子性的，这样就从操作系统层面解决了原子性问题，CAS通过自旋的方式去修改某个值，使用CAS时，有一个预期值和一个新值，当CAS操作启动之后，会先判断预期值与内存中该变量的值是否相等，相等则进行替换，否则CAS失败，这里的判断与修改是原子性的；CAS有个ABA问题，也就是在当前线程使用CAS进行某个数据的操作的时候，其它线程先将这个数据修改为其它的值，再修改回来，此时当前线程是察觉不到这种修改的，解决方法就是给数据加版本号，例如AtomicStampedReference

#### synchronized

- synchronized是用于在多线程环境下实现资源同步的一个关键字，在经过JDK6的优化后，synchronized一共有三种锁状态，偏向锁、轻量级锁、重量级锁，对应的锁标志位分别为01,00,10，用轻量级锁进行优化的基础是认为“对于绝大部分的锁，在整个同步周期内都不存在锁竞争”，也就是省去了进入临界区的过程，避免线程进入阻塞，用偏向锁进行优化的基础则是在无竞争的条件下彻底消除同步，在发生锁竞争时，会进行锁升级，并且不支持锁降级；偏向锁默认在程序启动后4秒后启动，偏向锁通过在对象头的markword中设置线程指针来达到获取锁的目的，当一个线程尝试获取某个对象的偏向锁的时候，会去判断对象头中的锁标志状态，如果是偏向锁状态101，则判断偏向的线程是否是当前线程，是则可以直接进入同步代码块，不是说明发生了锁竞争，就会在全局安全点
- 通过一对字节码指令monitorenter和monitorexit来实现，原理是每个对象都会有与之关联的一个ObjectMonitor对象，获取对象的重量级锁就是将ObjectMonitor中的owner设置为当前线程

#### volatile

- volatile是java提供的一个用于解决线程同步安全问题的关键字，它的作用是保证了可见性以及禁止指令重排序，它保证了可见性是通过缓存一致性协议完成的，它的内存语义在修改时就是会通知其它核将缓存行置为失效，按照MESI的做法就是将缓存设置为Invalid，这样当其它线程访问该变量时，会从主存中获取最新的数据；它禁止指令重排序是通过内存屏障完成的

#### AQS

- AQS可以说是整个Locks包的核心，AQS使用了模板方法的设计模式并使用CAS与阻塞的方式来获取锁，由AQS维护了一个CLH队列和一个表示锁数量的用volatile修饰的可见变量state，再让其它类通过AQS实现重入锁，读写锁，并发工具类这些用于实现线程并发与线程同步控制的类；比如acquire方法，这个是请求锁的主要方法，这个方法会先调用一次tracquire方法，这个方法就是由实现类决定具体实现细节，例如是否可重入，是否公平，可以说实际获取锁的细节是由实现类自己定义的，而AQS本身就是维护了一个骨架，当tryAcquire方法获取锁失败后，就会调用AQS的addwaiter方法将当前线程以指定的mode加入AQS的CLH阻塞队列，CLH是一个双向链表，头节点表示当前持有锁的线程，当持有锁的节点释放锁时，会唤醒下一个节点

#### ReentrantLock

- ReentrantLock是一个基于AQS的常用的可重入锁，在获取锁的时候会先直接尝试将锁标志位从0修改为1，如果修改成功，就将当前线程设置为获取锁的线程，否则就调用AQS的acquire方法

#### ReentrantReadWriteLock

#### Condition

#### 并发工具类 CountdownLunch、CyclicBarrer、Semaphore

- 在concurrent包中有一些用于并发流程控制的工具类，比如CountDownLunch，CyclicBarrer、Semaphore；CountDownLatch允许一个线程或多个线程等待其他线程完成操作，用来进行并发流程控制；CountDownLatch其实就是一个计数器，在初始化时初始化计数器的数量，然后每次调用CountDownLatch的CountDown方法都会使得计数器减一，直到为零后，调用了CountDownLatch的await方法的线程会恢复运行；CyclicBarrier是一个可循环使用的屏障，它让一组线程到达屏障时被阻塞，直到最后一个线程到达时全部唤醒，实现原理就是在初始化的时候会传入一个int类型的值并维护一个int类型的变量，当有线程调用CyclicBarrer的await方法时，会取得可重入锁并将这个变量减一，然后通过Condition的await方法进入阻塞，直到最后一个线程到达时变量减为零，调用breakBarrer方法也就是Condition的signalAll方法唤醒所有线程；CyclicBarrier在实例化时传入一个优先级更高的工作，等屏障打开后，会先让最后一个到达的线程执行完这个执行这个工作，再将其它线程唤醒；Semaphore管理一系列许可证。每个acquire方法阻塞，直到有一个许可证可以获得然后拿走一个许可证；每个release方法增加一个许可证，这可能会释放一个阻塞的acquire方法。然而，其实并没有实际的许可证这个对象，Semaphore只是维持了一个可获得许可证的数量，主要控制同时访问某个特定资源的线程数量，多用在流量控制。

#### ConcurrentHashMap

- ConcurrentHashMap是hashmap的使用于并发环境的版本，不同于hashtable仅仅在hashmap的基础上对方法加对象锁，concurrenthashmap使用了很多精妙的方式避免了不必要的线程阻塞，提高了吞吐量；ConcurrentHashMap在1.7中使用了分段锁的方式进行了锁细化，也就是hashtable是整个entry数组加锁，而现在将整个数组分为很多段，然后对这一段加锁；而在1.8中又进行了锁细化，在1.8中ConcurrentHashMap使用的是CAS+Synchronized，并尽量避免锁竞争；

- ##### sizectl

  - sizectl是一个volatile类型对象，在ConcurrentHashMap用来保存节点数，扩容线程数，特殊状态，扩容阈值；
  - sizectl为负数时，高16位保存容量，低十六位-1代表扩容线程数，-1表示正在初始化，0为默认值
  - sizectl为正数时，表示扩容阈值

  ##### 构造方法

  - ConcurrentHashMap采用懒加载的方式构造对象，在第一次插入值时才开辟空间

  ##### ConcurrentHashMap怎么保证线程安全

  - 在1.7版本中，CHM使用分段锁来保证线程安全，在1.8版本中，对锁的粒度进行了细化，使用Synchronized+CAS保证线程的安全性，维护了一个sizeCtl的全局volatile属性表示多种状态，并在多个地方使用到了DCL验证，同时添加了TreeBin对象保存红黑树的root节点，这样在进行红黑树的旋转时，不会因为根节点发生了变化而导致并发问题

  ##### ConcurrentHashMap初始化

  - ConcurrentHashMap使用initTable方法进行初始化，由于可能出现多个线程同时初始化的问题，所以使用sizeCtl=-1来表示线程正在进行初始化，所有初始化线程都使用CAS修改sizeCtl的值，修改成功的先进行一次DCL验证，然后初始化数组；其它线程会调yield方法让出时间片

  ##### ConcurrentHashMap插入值

  - ConcurrentHashMap实际插入值的方式其实与hashmap差不多，不过因为要保证线程安全，所以需要先做很多准备工作：
    1. ConcurrentHashMap在计算hash值的时候多与一个常量HASH_BITS进行了按位与，这个数实际上是int类型正数的最大值，二进制中首位为0，剩余位全为一，这样保证了计算出来的hash值一定为正数，避免与ConcurrentHashMap内置的`MOVED`、`TREEBIN`、`RESERVED`这3个负数hash冲突。
    2. 根据hash值计算的下标若为null，要使用CAS插入节点
    3. 若发现该下标对应的节点正在扩容，要调用helpTransfer方法帮助扩容线程进行扩容
    4. 在对应下标插入值时，要先取得该节点的对象锁
    5. 插入值完成后，如果节点数增加了，先判断是否要进行树化，然后需要进行总节点数量的统计，但如果仅仅使用一个变量来表示节点总数，那么多线程修改这个值也会出现严重的阻塞问题，这里对节点总数的计算采用了Fork/Join框架

  ##### put过程

  1. 调用put方法，put方法中回调putval方法
  2. 在putval方法中调用spread方法取得hash值（spread方法不仅使散列更均匀，同时也避免了hashcode值为负数，负数hash值在ConcurrentHashMap中有特殊含义）
  3. 进入死循环：
     1. 判断表有没有初始化，没初始化调用initTable方法初始化
     2. 初始化了则判断该下标处有没有节点，没有则cas插入节点
     3. 如果下标处已有节点，判断表是否在扩容，是则调用helpTransfer方法帮助扩容
     4. 表不在扩容则竞争该下标节点的监视器锁
     5. 竞争成功后再一次进行判断，防止该节点在竞争过程中被修改了
     6. 未被修改进入添加节点：
        1. 通过hash是否>=0判断是否为链表节点；是则循环遍历链表并比较各节点，直到替换值或在尾部添加节点；退出循环
        2. 判断头节点是否为TreeBin类对象，是则通过putTreeVal插入节点
        3. 通过binCount判断是否要进行树化；再判断此次插入值有没有插入新的节点，如果没有则直接返回旧节点而不会调用后面的addCount方法；然后退出死循环
  4. 调用addCount方法记录节点数
  5. 返回null

#### BlockingQueue

- BlockingQueue在普通的队列的基础上添加了阻塞式的加入数据put方法和获取数据take方法，这种阻塞式的添加与阻塞式的获取非常适合生产者消费者模式，比如线程池中就用到了BlockingQueue存储工作；Blocking的主要实现类有ArrayBlockingQueue、LinkedBlockingQueue、PriorityBlockingQueue、SynchronousBlockingQueue等，ArrayBlockingQueue使用一个ReentrantLock来解决线程同步问题，当线程需要从ArrayBlocking中获取数据的时候，需要先获取这个锁，这意味着ArrayBlocking实际上是无法做到并发的读写操作的，所以比较适用于读取或者写入只有一方面密集的操作，另外，使用数组来实现队列的基本都是使用的循环队列，这意味着用数组实现的队列是无法扩容的，所以在初始化队列之前要选择一个合适的大小；LinkedBlockingQueue底层使用的数据结构是链表，与ArrayBlockingQueue不一样的是，LinkedBlockingQueue实现了锁分离即使用了两个可重入锁分别维护了添加与获取，所以LinkedBlockingQueue是可以读写并发的，具有更高的吞吐量，但是由于需要不停的构建节点对象以及释放节点对象，在高并发的场景中对于GC的压力会更大一点；
- 如果需要改成双锁，即可能有多个线程同时对队列进行操作，那么ArrayBlockingQueue就需要进行一点点改变：首先，对于记录元素数量的count必须改为原子性修改，所以要不像LinkedBlocking一样使用原子操作类，要不在用volatile修饰count，并在修改的地方都使用cas修改；其次，入队操作和出队操作在唤醒阻塞线程时，就像synchronized的notify要先获得锁一样，它也必须要先获取锁，所以转成双锁之后，对比原来的存取操作，需要多竞争两次。一次是Atomic变量的cas操作，另一次是获得另一把锁的通知操作。可能这部分的损耗，已经比并发存取带来收益更大。

- LinkedBlockingQueue的较大一部分时间需要构造节点，导致较长的等待。所以同时存取有较大优化。

#### ThreadPoolExecutor

- ThreadPoolExecutor是一个容器，主要维护了一个用于存放待完成的工作队列和一个存放正在运行的所有线程的工作集合，当程序需要使用多线程执行某个任务的时候，就不需要再新启动线程，只需要将任务放到线程池中，线程池会分配线程来执行这个任务，具体流程是当核心线程数小于最大核心线程数时，会创建新的线程来执行这个工作，当工作线程数大于最大核心线程数时，会将线程放入阻塞队列中，如果阻塞队列已满，就会判断当前的工作线程数是否大于等于最大线程数，是的话执行拒绝策略，否者就创建一个新的线程来执行这个任务，所以任务的执行顺序并非绝对公平的，当线程执行完当前任务后，会从阻塞队列中获取工作继续执行，直到阻塞队列为空就进入阻塞，阻塞时间达到最大空闲时间后，判断当前线程数是否大于最大核心线程数，是则销毁线程，另外，如果设置了allowCoreThreadTimeOut为true,即使是小于最大核心线程数也会进行线程的销毁；使用线程池利于管理线程，同时由于提前创建好了线程，当执行任务的时候节省了创建线程的时间，提高了响应速度，使用线程池达到了复用线程的目的，避免了频繁的创建和销毁线程，节省了资源消耗；获取线程池总共有五种方法，推荐使用原生的线程池构造

### redis

#### redis数据结构

- redis数据对象有五种基本数据类型String、List、Hash、Set、ZSet和三种特殊geospatial、Hyperloglog、BitMap，我比较熟悉这五种基本的数据类型；redis 的这五种数据类型都有两个或两个以上的底层数据结构，根据元素数不同或存储值状态使用不同的数据结构，比如String有INT、EMBSTR、RAW，List使用ZIPLIST、LINKEDLIST，Hash使用ZIPLIST、HT，Set使用INTSET、HT，ZSet使用ZIPLIST、SKIPLIST，可以使用OBJECT ENCODING来查看对象的底层类型；string对象存储整数值的时候会用INT类型来存储，此时可以使用INCRE和DECRE这些命令进行加减，但是修改为字符串之后就会升级为RAW类型，RAW类型和EMBSTR类型都是SDS类型的字符串，都使用redisObject结构和sdshdr结构来表示字符串对象，不同的是embstr是专门用于保存短字符串的优化方案，长度39以下的默认使用这个数据结构，就是底层的存储方式有点不同，将redisObject和sdshdr分配在一块内存空间中，而raw是调用两次内存分配函数来分别分配，所以embstr实际上是不可变的，一旦进行修改就会升级为raw类型，然后sdshdr中会保存三个属性，free、len、buf，其中free表示buf数组未使用的长度、len表示buf已使用的长度、buf数组就是实际存储字符的char数组，sds本身是二进制安全的，它不会对存储的字符做任何转义，并且可以使用len属性定位到字符串的结尾，所以不需要像c字符串一样使用空字符'\0'结尾，但是redis字符串也会用\0去结尾，这样就可以使用部分的C字符串的函数库，这个free代表字符串的空闲的空间，因为redis中的字符串修改比较频繁，所以在扩容时会预分配一些空间以供后续使用，默认字符串扩容后大小1MB一下时预分配字符串大小的空间，1MB以上后分配1MB，使用SDS字符串的好处是杜绝缓冲区溢出减少内存重分配次数、二进制安全；

#### redis持久化

- redis是完全基于内存的，它的持久化其实就是使用日志保存当前数据库的数据，redis的持久化方案有两种，一种是RDB，另外一种是AOF；RDB使用物理日志也就是二进制文件来进行数据存储，使用二进制文件的好处是恢复比较快，但是缺点是每次备份消耗比较大，所以不适合用于频繁的持久化；RDB使用save和bgsave来进行持久化工作，save指令会阻塞服务器，所以一般不会使用这个，bgsave指令会fork一个子进程进行RDB去创建RDB文件，redis可以继续处理客户端的命令请求；RDB还可以在配置文件中配置自动间隔性持久化，比如在6379.conf中配置save 300 5，就是指在300秒内发生过5次写操作就会进行持久化，然后这配置可以配置多个，在启动redis服务的时候，会将这些配置放进一个saveparam数组中，然后在redis还有两个参数dirty计数器和lastsave属性，然后在redis里面有两种事件，文件事件和时间事件，文件事件就是对客户端请求进行处理，时间事件就是一些定时任务和周期任务，RDB的间隔性保存就是有一个周期性函数serverCron做的，它有一项工作就是遍历saveparam数组，根据dirty和lastsave参数来判断是否要进行bgsave；而AOF就是使用的逻辑日志来进行持久化，就是将写命令对应的协议内容追加到AOF的日志文件中，需要的时候再按照这个日志文件执行一遍就可以做到恢复数据的作用，与mysql的innodb一样，redis也有AOF的缓冲区用来提高IO效率，这个缓冲区叫做aof_buf，每次有写命令就会追加到缓冲区的末尾，然后根据设置的刷新频率来将缓冲区的内容刷新到磁盘，并使用flushAppendOnlyFile函数进行写入并强制同步，这个频率的设置有always每次执行这个函数都刷新、everysec，默认的，超过一秒就会刷新，no，将aof_buf缓冲区的内容写入AOF文件，但不强制同步，等操作系统自动刷新；

#### AOF重写

- AOF持久化的原理是将写操作追加到AOF日志文件，所以当redis运行一段时间后，AOF文件可能会产生很多冗余，既浪费磁盘空间，又不利于使用AOF文件进行恢复，所以就有了AOF重写操作，其实AOF重写跟原来的AOF文件是没有什么关系的，它的实际操作与RDB有点类似，只不过是使用逻辑日志的方式记录当前数据库的状态，启动AOF重写可以使用BGREWRITEAOF进行后台aof重写，更bgsave一样fork了一个进程来进行这个操作，并且重写操作是在原数据库的副本上进行的，这样就不会出现线程同步问题，然后由于重写与服务器进程是并发执行的，所以使用了一个aof重写缓冲区来保存重写期间的写命令，并在重写完之后追加到新aof文件的末尾，并使用新的aof文件替换掉原aof文件，这样就完成了aof文件的重写

#### redis的文件事件

- redis的文件事件就是指redis服务接收并处理客户端请求，redis基于Reactor模式开发了自己的网络事件处理器叫做文件事件处理器，这个处理器使用了IO多路复用程序来同时监听多个套接字，并根据套接字执行的任务来给这些套接字关联不同的文件事件处理器，当被监听的套接字准备好连接应答、读取、写入、关闭等操作的时候，与操作对应的文件事件就会产生，然后文件事件处理器就会调用套接字之前关联好的事件处理器来处理这些事件；IO多路复用程序可以监听多个套接字的AE_READABLE和AE_WRITABLE事件，当套接字变得可读也就是客户端执行写操作或者执行close操作的时候，就会产生AE_READABLE事件，客户端执行读操作的时候，套接字就会产生AE_WRITABLE事件，IO多路复用程序会优先执行READABLE事件；redis的文件事件处理器主要有连接应答处理器acceptTcpHandler函数、命令请求处理器readQueryFromClient函数、命令回复处理器sendReplyToClient函数；

- redis启动后会将服务的监听套接字的AE_READABLE事件与连接应答处理器关联，当客户端发送连接请求之后，监听套接字就会产生AE_READABLE事件，然后连接应答处理器就会对客户端的连接请求进行应答并创建客户端套接字，然后将这个客户端套接字的AE_READABLE事件与命令请求处理器进行关联，这样后面客户端发送读写操作的时候，都会由命令请求处理器进行执行，执行命令之后会产生对应的命令回复，所以服务器会将客户端套接字的AE_WRITABLE事件与命令回复器关联，当客户端尝试读取命令回复的时候，客户端套接字就会产生AE_WRITABLE事件，触发命令回复处理器执行

#### redis的时间事件

- redis的时间事件由三个属性组成，全局id、到达时间when、时间事件处理器timeProc，redis根据时间事件处理器的返回值分为周期性时间事件和定时时间事件，时间事件处理返回AE_NOMORE的就是定时时间，不会再被执行，返回其它整数值的就是周期性时间事件，服务器会根据这个返回值更新事件的when，并以这种方式一直运行下去；服务器将所有的时间事件放在一个无序链表（不按时间排序，但是会按id排序）中当时间事件执行器运行的时候，就遍历整个链表，执行所有已达到时间的时间事件，一般情况下只使用serverCron这个时间事件
- serverCron是维护redis运行的重要函数，以定期的方式运行，它会在服务器运行期间间隔性执行，主要工作有：更新服务器统计信息、清理过期键值对、关闭和清理连接失败的客户端、尝试进行AOF或RDB持久化、主从定期同步、集群定期同步和连接测试等

#### redis的复制

- redis的复制可以用来做数据备份、故障恢复、读写分离、负载均衡等，redis的复制与mysql的很像，就是将RDB文件发送给从机，然后从机根据这个日志文件进行恢复操作，具体实现是：从服务器的配置文件中配置slaveof 加主服务器的ip和端口，如果主服务器设置了密码还要配置主服务器密码，然后用客户端连接从服务器，发送slaveof指令，此时就会开始复制：从服务器会向主服务器发送PSYNC命令，主服务器收到PSYNC命令后执行BGSAVE命令，在后台生成一个RDB文件并且使用一个缓冲区记录从此刻开始的所有写命令，然后将生成的RDB文件发给从服务器，从服务器接收并载入这个RDB文件，并且将直接的数据库状态更新到主服务器执行BGSAVE命令时的状态，然后主服务器再将缓冲区里面的所有写命令发送给从服务器，从服务器执行这些写命令，并更新数据库状态；后面再使用复制积压缓冲区来维护主从同步

#### 怎么使用redis做缓存

- 首先要导入依赖jedis包和spring-data-redis包，然后配置配置redis连接工厂jedisConnection以创建redis连接，再配置redis连接池用于获取redis连接，再使用JedisTemplate来进行数据的操作，也可以自封装一些需要用到的redis操作

#### redis过期键的删除策略

- redis可以为键设置过期时间，redis

#### 缓存三大问题与解决方案

- 缓存三大问题分别是缓存穿透、缓存雪崩、缓存击穿；实际上三个缓存问题的本质都是有过多的请求穿过缓存层到达了数据库层，导致数据库可能因为产生过多请求而崩溃；缓存穿透是指查询一个一定不存在的数据，由于这个数据一定不存在于缓存，那么就会去访问数据库，当这种查询被恶意利用的时候，就会破坏数据库的性能，解决方案有空值缓存和布隆过滤器，前者是在服务返回不存在的键对应的数据后，在缓存层放上一个对应的空值缓存，这样下一次访问就会在缓存层返回空值，这样做的缺点是当恶意攻击者使用随机数例如查询uuid这样的随机数的时候，空值缓存不仅无法有效拦截，而且可能导致正常的热点数据被挤出缓存层，可以使用布隆过滤器进行拦截，布隆过滤器通过位图记录当前数据库存在的数据，当查询缓存层发现数据不存在之后，查询在请求数据库之前会通过布隆过滤器判断是否存在数据库，不存在则直接返回null；缓存雪崩是指大量的缓存由于缓存服务器宕机，缓存有效期同时失效等问题大量失效，解决方案是做redis集群和在一定范围内设置随机数缓存有效期；缓存击穿是指某条热点数据同时被大量查询，比如预售商品、优惠券、微博热搜等，解决方案是数据预热，设置有效期为永久，并且再设置数据库连接池，只有获取到连接的查询才能访问数据库

### MySql

#### mysql语句的执行流程

- 首先客户端通过mysql服务器的连接器基于TCP获取连接，这个过程会有身份认证和权限验证等，然后提交sql语句；如果服务器启动了查询缓冲，会通过查询缓存查看是否有该sql语句的缓存，但是这个缓存特别容易失效，所以默认不开启，并且在mysql8中移除了；之后会通过分析器进行词法分析和语法分析并生成解析树，并通过预处理的方式检查语义，然后通过优化器生成最佳的执行计划；最后通过执行器在存储引擎完成查询并返回数据，

####  事务执行流程

1. 启动事务，set autocommit = 0； 或者显示启动一个事务 begin或者start transaction
2. 启动事务后并不会为当前事务分配事务id等，而是延迟到第一次操作，第一次执行操作后，会为当前事务分配一个事务id，并生成一个readview，这个readview是实现事务隔离的关键，readview中保存了
   - m_ids，当前正在执行的所有事务
   - min_trx_id，当前正在执行的所有事务的最小事务id
   - max_trx_id，下一个分配的id
   - creator_trx_id，创建这个读视图的事务id
3. 事务更新某条数据时，会先获取这条数据的行锁，然后将更新前的数据写入undo log，并构建指向该 undo log的指针DB_POOL_PTR，并产生redo log
4. 当再次访问某条数据的时候，就会通过这个数据行的隐藏字段DB_TRX_ID与当前事务的读视图，然后再根据不同的事务隔离级别，就可以访问到数据的不同版本，举个示例吧：
   1. 有t1，t2，t3三个事务，事务t1修改了id=1这一列并且提交了，那么这一列的DB_TRX_ID就是t1的事务id，然后t2，t3依次开启，并依次获取读视图，那么因为这两个事务的min_trx_id是大于id=1这一列的DB_TRX_ID，所以这两个事务都对t1之前做的修改可见，可以读到最新版本的数据，然后t2再去修改id=1这一列，这个时候会获取这一列的行锁，t3修改不了，但是可以读，innodb使用mvcc实现了无锁读，这个时候就会根据不同的隔离级别有不同的反应：如果是RU级别，不会去判断行版本的可见性，直接获取当前行的记录，也就是最新的记录；如果是RC级别，会生成一个新的readview，然后进行判断行记录的DB_TRX_ID，这个DB_TRX_ID小于min_trx_id是可见的，等于当前事务id也是可见的，大于等于max_trx_id就是不可见的，在min_trx_id和max_trx_id之间，就用二分查询看它在是否在活跃列表中，在则不可见，否则可见，这也就是为什么提交了的事务在RC级别是可见的；然后如果是RR级别就根据之前的readview进行可见性判断；如果不可见就根据DB_ROOL_PTR向上一个版本的数据进行可见性判断
5. 事务提交，二阶段提交redo log和bin log

#### mysql事务的MVCC

- mysql使用undo log完成了mvcc，其原理是每个数据行都有两个隐藏的字段DB_TRX_ID和DB_ROLL_PTR，前者为该行上一次修改的事务id，后者为指向当前行的最后一个undo log的指针；事务在第一次操作的时候会生成一个readview，这个readview中会保存几个重要属性：创建readview时正在执行的所有事务的事务id列表m_ids，正在执行的最小事务min_trx_id，待分配的下一个事务max_trx_id,创建这个视图的事务creator_trx_id，通过数据行的隐藏字段和readview，再配合undo log，就可以实现mvcc了；mvcc译为多版本并发控制，其实就是用undo log储存了事务对数据做的修改，这样就可以实现非锁定读，其它事务可以通过undo log读到数据行的历史版本；先讲一下写操作，写操作会获取数据行的写锁，然后将该写操作对应的方 向 操作写入undo log的结尾，修改行记录，修改行记录的DB_TRX_ID为当前事务id，修改DB_ROOL_PTR为新的undo log的尾端，这样就完成了修改操作；然后再讲一下读操作，先讲一下默认的RR隔离级别，RR隔离级别其实就是可以读到创建事务时创建的快照上面的数据，具体实现就是会判断行记录的DB_TRX_ID是否小于当前事务的readview的min_trx_id或者等于当前事务id或者小于max_trx_id但是不在列表中的id，是则可以访问，否则会通过DB_ROOL_PTR向上找历史版本，也就是说，如果这个行的修改的事务在当前事务创建之前提交了，就可以读取到，否则要查看历史版本；而RC级别就是每次读都会新生成一个readview，所以在这次读之前提交的所有事务做的修改都可以被读取到，这也就是为什么RC级别未解决可重复读问题，然后RU级别根本不会去判断而直接读取行记录的当前版本，所以会产生脏读问题

#### mysql结构

#### sql执行流程

- 常用的：FROM -> ON -> JOIN -> WHERE -> GROUP BY -> HAVING -> SELECT -> DISTINCT -> ORDER BY -> TOP

### JVM

#### JVM的主要组成部分及作用

- JVM包括两个子系统和两个组件，两个子系统是类装载系统和执行引擎，两个组件是运行时数据区和本地接口；类装载系统根据类的全限定名将class文件加载进java虚拟机，执行引擎执行字节码指令，本地接口与本地方法库交互，运行时区域就是JVM的内存，维护jvm的正常运行；编译器将Java代码转化为字节码文件，然后用类加载器加载到内存中，放在运行时数据区的方法区中，然后由执行引擎将字节码翻译成底层系统指令由cpu执行，这个过程需要使用本地接口调用其它语言来实现功能

#### 深拷贝与浅拷贝

- 其实还有一个引用拷贝，引用拷贝就是类似于一个赋值操作；而浅拷贝和深拷贝都会分配一块新的对象大小的内存区域，不同的是浅 拷贝不会将对象内的引用类型再拷贝，而仅仅是进行了一下引用拷贝，而深拷贝会递归的拷贝对象内的引用类型；举个简单的例子就是有个类Student有成员int类型的id和Address类型的address，并且有一个这个类的对象被s1引用，然后s2、s3、s4分别是s1的引用拷贝、浅拷贝、深拷贝，那么s1与s2的值实际上是同一个对象，在内存中是同一块内存区域，而s1与s3的值代表是两个不同的对象，属于两块不同的区域，但是他们中address的值是同一个对象，而s1与s4也是两个不同的对象，且address属性也是两个不同的对象

#### 运行时数据区

- 程序计数器解释器工作时就是通过改变这个计数器的值来选取下一条需要执行的字节码指令，本地方法栈存放本地方法的方法栈帧，虚拟机栈存放程序方法的栈帧，每个方法的栈帧中都存有本地变量表、方法出口、操作数栈、动态连接等信息，在hotspot中将本地方法栈和虚拟机栈合并为一个栈；堆存放的是对象实例，几乎所有的对象实例都是存放在堆空间中，垃圾收集器主要清理的区域也是堆空间；方法区用于存放被虚拟机加载的类型信息、常量、静态变量、即时编译器编译后的代码缓存等数据

#### 对象的创建过程

- 当执行到一条new指令之后，java虚拟机会根据new 指令之后的参数尝试在常量池中定位一个类的符号引用，并检查这个符号引用代表的类是否已经被加载、解析和初始化过，否则就要进行类加载；类加载检查通过后，给这个对象根据类信息分配空间，并且给所有的属性初始化零值，并设置对象头信息，最后执行类的init方法

#### 对象内存分配过程

- 对象内存分配有两个方式，指针碰撞和空闲列表，使用哪种分配方式与内存空间的分布有关，如果空间是绝对规整的，就可以使用指针碰撞的方式，如果内存不是绝对规整的，那么就只能使用空闲列表的方式，而空间是否规整是否规整又与垃圾收集器使用的垃圾收集算法是否有空间压缩能力有关，例如serial、parnew这种使用标记-复制算法的就具有空间压缩能力，而CMS这种标记-清除能力的就不具有空间压缩能力

#### 根节点枚举

- 判断对象是否死亡有根据引用计数算法和可达性分析算法这两种方法，根据引用数判断对象的存活需要考虑许多特殊情况，例如相互引用这样的问题；在hotspot中的垃圾收集器都是使用可达性分析算法，即判断对象是否直接或间接的被根节点对象引用；根节点对象一般有被栈引用的对象，被方法区的静态变量和常量引用的对象，hotspot中有一个OopMap的数据结构保存了哪些地方存在着对象引用，这样就不用遍历整个执行的上下文和全局；OopMap不会在每条指令都生成，而是在安全点才会生成，安全点有循环跳转、方法跳转这些；并且在并发的可达性分析算法中使用增量更新和原始快照来解决对象消失问题

#### CMS垃圾收集器

- CMS垃圾收集器是基于标记-清除算法的老年代垃圾收集器，它的目标是达成最短停顿时间，因此它的垃圾回收过程会尽量与用户线程并发完成；CMS的垃圾回收过程分为四个阶段：初始标记，并发标记，重新标记，并发清除；初始标记需要stop the world（基本上所有的垃圾收集器在这个过程都需要stw），完成的工作是表示所有的GC ROOTs，并发标记与用户进程并发执行，主要工作就是标记所有可以被GC ROOTS直接或间接应用的对象，由于这个过程中用户活动可能会改变对象的引用关系，可能产生对象消失问题，所以在这个过程中还要基于写后屏障来完成增量更新的对象的记录，即记录新的由黑色指向白色节点的引用；重新标记会停顿用户线程，根据并发标记过程中记录的节点再进行剩余对象的扫描与标记；并发清除阶段就是根据之前的标记信息将所有没有被引用的对象进行清除；
- CMS垃圾收集器本身有一些缺点，第一个就是它是基于标记清除算法的，这个算法的优点是不用移动对象，缺点也很明显，会产生碎片空间，当为某个大对象分配空间而找不到适合的连续空间的时候，可能会产生提前的full GC；其次是CMS的清除阶段与用户线程并发进行，在这个过程中用户线程产生的垃圾在本次垃圾回收过程中是无法清除的，这部分垃圾叫做浮动垃圾，所以需要预留出一部分的内存空间以供用户使用，当在并发清除阶段用户空间不足时，会产生并发失败，由serial old来进行一次完全停顿用户线程的full GC

#### Garbage First垃圾收集器

- 

#### 类加载机制

- JVM的类加载过程为加载、验证、准备、解析、初始化，也可以将准备、解析、初始化统称为连接，其中加载过程做的事就是根据类的全限定名来取得该类的二进制流，在将这个二进制流所代表的静态存储结构转化为方法区的运行时接口，然后在方法区生成代表这个类的Class对象作为这个类的访问入口；验证阶段是为了保证字节码文件没有被恶意篡改，主要的工作有文件格式验证、元数据验证、字节码验证、符号引用验证；准备阶段就是为类的静态变量分配空间并赋零值；解析阶段将常量池的符号引用转换为直接引用；初始化阶段会执行类构造器init方法

#### 双亲委派模型以及被三次被破坏

- 双亲委派模型是指JVM搭建了一种以启动类加载器（bootstrap Class Loader）为顶层、扩展类类加载器（Extension Class Loader）为第二层、应用程序类加载器（Application Class Loader）为第三层的这样带着层级关系的结构，当底层的类加载器执行类加载请求的时候，会优先将这个类加载请求委托给高层的类加载器执行类加载请求，这样一层一层的向上委托，只有当高层的类加载器无法完成类加载的时候，才会由自己完成类加载请求，这样使得某个类的加载可以确定是由哪个类加载器完成的，使得类随着加载它的类加载器具备了层次的特性，保证了JVM中类的唯一
- 双亲委派模型第一次被破坏是由于在双亲委派模型出现之前已经有一些类加载器的出现了，这部分类加载器没有按照双亲委派机制进行实现，即它们重写了ClassLoader的loadClass方法，所以ClassLoad的loadClass方法就没有办法使用final进行修饰，所以出现了一个findClass方法，且建议使用这个方法而不是修改loadClass方法，当父类加载器无法完成加载请求时，会使用findClass完成当前类加载器的加载逻辑；第二次被破坏是由于首先于先让父类加载器进行加载的这个原则，某些时候会出现父类加载器请求子类加载器执行加载请求的场景，比如SPI的加载，SPI是属于核心代码的，由它的一些实现例如JDBC等却是由应用程序类加载器来加载，解决方案是使用线程上下文类加载器进行加载；第三次被破坏是模块化系统的出现，当收到类加载请求，会先判断该类在具名模块中是否有定义，如果有定义就自己加载了，没的话再委派给父类。

### spring

#### spring ioc

- ioc翻译为inverse of control，也就是控制翻转，原本当我们在程序中使用到某个对象时，需要在程序中使用硬编码的方式去new这个对象，而使用spring容器则变成了由spring 容器统一创建，且对象的整个生命周期都由spring容器统一管理，再由spring容器通过DI的方式注入到对象中以供对象的使用；并且spring容器在容器创建过程中和对象创建过程中都提供了大量后置处理器来进行扩展功能，比如可以自定义注解，比如使用aop功能；

#### spring容器的创建过程

- spring容器的创建过程其实就是在使用BeanDefinitionReader解析对象，并且将对象的属性封装为BeanDefinition对象，放到ApplicationContext的内部容器beanFactory的beanDefinitionMap中，以供后期对象的创建使用，在初始化spring容器的时候，spring使用到了模板工厂模式，并预留了许多扩展的地方比如postProcessorBeanFactory方法和onFresh方法，同时还可以通过条件BeanFactoryPostProcessor和BeanPostProcessor来对功能进行增强

#### 使用spring框架的好处

- 方便解耦，便于开发；支持aop编程；声明式事务的支持；方便集成各种优秀框架；提供了很多高度集成工具

#### bean的生命周期

#### 循环引用问题

- spring通过提前暴露对象的方式来解决循环依赖的问题，也就是将对象的实例化与初始化分开；spring有三个缓存，singletonObjects，earlySingletonObjects，singleObjectFactories，通常叫做一二三级缓存，其中一级缓存会存放完全实例化好了的对象，一般情况下getBean就是从一级缓存中获取对象的过程，三级缓存会存放对象的实例工厂，也就是说，通过三级缓存的对象工厂可以获取到该对象，其获取过程就是调用工厂的getObject方法，这个getObject方法是在将工厂对象添加到三级缓存时放入的一个lamda表达式

#### AOP代理的实现

#### spring事务

- spring的事务有7种传播方式和5种隔离级别，分别用Propagation和Isolation设置，七种传播方式分别是REQUIRED、SUPPORTS、MANDATORY、REQUIRED_NEW、NOT_SUPPORTS、NEVER、NESTED，五种隔离级别就是默认级别加RU、RC、RR、S；Spring通过三个接口来完成对事务的支持，TransactionDefinition、PlatformTransactionManager、TransactionStatus，其中TransactionDefinition会封装事务的一些属性，就像IOC可以通过Component注解获取类的描述信息并封装到BeanDefinition中一样，这个也是

### spring mvc

#### spring mvc执行流程

1.用户发送请求至前端控制器DispatcherServlet(也叫中央处理器).
2.DispatcherServlet收到请求调用HandlerMapping处理器映射器
3.处理器映射器找到具体的处理器（可以根据xml配置、注解进行查找），生成处理器对象及处理器拦截器（如果有则生成）一并返回给DispatcherServlet.
4.DispatcherServlet调用HandlerAdapter处理器适配器。
5.HandlerAdapter经过适配调用具体的处理器（Controller,也叫后端控制器）。
6.Controller执行完成返回ModelAndView.
7.HandlerAdapter将controller执行结果ModelAndView返回给DispatcherServlet.
8.DisPatcherServlet将ModelAndView传给ViewReslover视图解析器。
9.ViewReslover解析后返回具体View.
10.DispatcherServlet根据View进行渲染视图（即将模型数据填充至视图中）。
11.DispatcherServlet响应用户。

#### spring mvc拦截器

- mvc拦截器的实现需要先编写一个拦截器类实现HandlerInterceptor接口或WebRequestInterceptor借口或继承它们的实现类，然后在spring mvc的配置文件中配置interceptor，主要配置一下拦截的访问路径与不拦截的路径还有拦截器类

### mybatis

#### mybatis初始化的过程

- mybatis在SqlSessionFactoryBuilder的build方法使用XMLConfigBuilder将配置信息进行解析，然后封装到Configuration中以便使用，比如别名，接口映射等，然后将这个Configuration传给新建的SqlSessionFactory，在使用SqlSessionFactory打开一个会话的时候，可以指定启动的执行器类型、是否自动提交等，并将SqlSessionFactory的Configuration传给新打开的会话

#### mybatis的一级缓存二级缓存

- 简单来说，

#### Executor执行的过程

- 执行查询的时候，会先从二级缓存查找数据，不存在则从一级缓存查找数据，也不存在则调用queryFromDatabase从数据库中查找数据，具体过程是先创建PrepardStatementHandler，获取数据库连接，对mappedStatement进行预处理，就是调用JDBC的PrepareStatement;在创建PreparedStatementHandler对象时会为这个对象实例它的两个成员变量ParameterHandler和ResultSetHandler，后续使用ParameterHandler对参数进行处理，使用ResultSetHandler对结果集进行处理

#### mybatis的工作原理

- mybatis执行过程就是首先通过resource对配置文件进行解析，然后使用XMLConfiguration的parse方法将所有的配置信息封装到了一个Configuration这样的类中，再通过SqlSessionFactoryBuild方法取得一个SqlSessionFactory的实现类DefaultSqlSessionFactory，这样就可以通过这个SqlSessionFactory来取得SqlSession，SqlSession中有两个重要的成员，一个是配置信息Configuration，另一个是执行器Executor，Executor才是真正执行sql语句的对象，它帮助我们简化了例如获取连接，释放连接这样的操作，使我们在开发中可以专注于业务代码；然后在调用getMapper的时候会使用JDK动态代理来为接口创建一个实现类，并将mapper标签的每个mapper接口
- 使用sqlsession的增删改查方法其实就是在调用

#### 如何自定义类型转换器

- 首先要编写一个类型转换器，这个类继承BaseTypeHandler，然后重写

#### resultMap

#### mybatis的一级缓存与二级缓存

### Linux

#### 某些常用的命令

- cd打开目录，mkdir创建目录、ls当前目录下的文件及目录、cwd当前目录、cat打印文件内容、touch创建文件、cp复制文件、mv移动文件、rm删除文件、ps查看进程信息、top查看所有进程信息及系统资源占用情况

#### ps与top的区别

- ps打印的时进程信息的快照，而top是实时打印进程信息，top还可以看到系统的内存占用等系统资源的情况，还可以操作进程例如修改进程的优先级

### 操作系统

#### 进程间通信的方法

- 主要有管道、信号量、共享内存、消息队列、等

#### 进程调度算法

- 进程调度算法有先来先服务、短作业优先、优先级调度、时间片轮转、高响应比、多级反馈队列；分抢占式与非抢占式

### 计算机网络

#### 连接的本质

- 我所理解的连接的本质是资源的开辟与约定的达成，拿tcp的连接来说，连接就是发送缓冲区与接收缓冲区的开辟，ip地址与序列号的交换，应用层的话可能就是端口的占用，比如一个socket能唯一确定一个连接

#### TCP

- tcp是一个可靠的，面向连接的基于字节流的运输层传输协议；使用tcp协议进行数据传输前要进行三次握手，结束数据传输时要进行四次挥手，tcp根据三次握手和重传机制来达到可靠传输的目的，通过滑动窗口的方式来加快数据传输效率，通过拥塞控制来进行传输速率的控制；

##### 讲一下TCP的三次握手和四次挥手，为什么要等待2ML

- 首先稍微讲一点tcp的报文头部，tcp报文头部在进行连接中主要用到的有16位的源端口和目标端口，32位的序号syn和确认应答号ack，另外还有标志位syn和标志位ack各占1位；在连接过程中，客户端向服务端发送一个标志位syn为1，序号syn随机生成的tcp报文，服务端在收到这个报文后，会将连接存储到半连接队列，然后向服务端发送一条标志位ack为1和标志位syn为1的报文，之后客户端再向服务端发送一条ack为1的报文，服务端接收后，将连接移出半连接队列，放入全连接队列；这样，三次握手的过程就完成了；关于为什么要使用三次连接，我的理解是，tcp建立连接的目的是保证可靠性，而怎样才算可靠的能，当双方都能确定自己可以接收到对方发出的报文且对方可以接收到自己发出的报文的时候，这个连接就是可靠的，服务端第一次接收到syn报文时，可以确认自己可以接收到对方发出的报文，所以需要回一条报文让对方知道自己可以接收到对方发出的报文，于是这条报文有ack标志，同时服务无法确认自己发出的报文对方能否接收到，所以需要对方在接收到这条ack报文后在发一条报文过来，所以这条报文有syn标志，这也就是第二次握手的报文为什么是syn+ack，而第三次报文是ack报文；以客户端请求断开连接来讲一下四次挥手的过程，客户端向服务端发送一条标志位fin为1的报文表示断开连接请求，服务端接到请求后，会返回一条ack报文表示已接到断开连接请求，但是与连接时不同，此时是处于已经建立了连接的状态，服务端可能还有要往客户端发送的数据，所以服务端不会将端开连接的请求与ack请求一起发送，而是在之后确定无任务之后再发一条fin报文给客户端，然后客户端在接收到fin报文后返回ack请求，并在2ml后关闭连接，而服务端则在接收到2ml后立刻关闭连接；关于等待2ml有两个原因，一个是ack报文可能需要重传，2是防止已经失效的报文，等待2ml能够保证在这个端口建立新的连接时，所有上一次连接产生的在网络中报文都已经消失了

##### 讲一下tcp的重传机制

- tcp有两种重传机制，一个是超时重传，也就是一段时间后还没接受到对方返回的ack报文时，会重新发送上一条报文；还有一种是快速重传，这种是开启了滑动窗口后会与超时重传一起使用的重传机制，启动了滑动窗口后，客户端会根据服务端ack中的窗口大小连续发送多条报文而不需要等待服务端的ack报文，由于接受到的报文可能是乱序的，所以服务端会使用缓冲区来保存这些报文，并且客户端发送的报文在没有接受到ack回应前，也需要使用缓冲区来保存报文，服务端根据报文的syn序号将报文进行重排序，并返回当发现缺失某个报文包的时候，后面序号的报文的ack包的ack都会是这个报文包，当客户端收到三次冗余的同一个ack包时，会触发快速重传，打断超时重传，并马上重传该报文包，还有一种SACK重传，就是服务器会通过sack选项返回携带所需要的报文序号及范围；

##### 讲一下拥塞控制

- tcp使用滑动窗口来加快传输速率而不必等待对方的ack应答报文，而这个发送速率的限制同时受两个方面影响，一个是当前的网络状态，一个是接收方的接收能力，发送方可以发送的报文大小会根据接收方返回的ack报文的接收窗口大小rwnd来发送，但是由于目前的网络状态还未知，因此，即使接收方可以接收很大的报文长度，发送方也不能连续发送很多个报文，具体发送窗口swnd还需要根据拥塞窗口cwnd的大小进行调整，拥塞窗口的大小是根据网络情况而定的，根据几个拥塞算法进行动态调整，发送窗口的大小会在拥塞窗口和接收窗口取最小值；拥塞窗口的大小会根据四个算法动态调整：慢开始、拥塞控制、快速重传、快速恢复；拥塞窗口大小维护的原则是没有出现拥塞就调大一点，出现了拥塞就调小一点；慢启动当拥塞窗口小于慢开始门限的时候，第一次发送报文大小为拥塞窗口大小，如果没有发生拥塞，则将拥塞窗口大小翻倍，直到拥塞窗口的大小大于慢开始门限，改用拥塞避免算法；拥塞避免则是每次拥塞窗口的大小加1，直到发生重传之后，将慢开始门限设置为当前发送窗口的一半，并重新开始慢启动，进入快速恢复阶段，在快速恢复阶段，每次收到重复的ACK会将拥塞窗口大小加一，收到非重复的ACK时，会直接将拥塞窗口的大小设置为慢开始门限大小，然后进入拥塞避免阶段；tcp还使用了快速重传机制来加快发送方进行传输速率的调整，快速重传指当接收方接收到某个报文时，如果有这个报文的序号之前的报文未收到，就会不针对这个报文返回ack应答，而是以缺失的报文作为ack应答，当发送方收到三次冗余的同一个报文的ack应答时，会中断超时重传，触发快速重传；

#### UDP

- UDP是无连接的，尽最大努力交付报文的基于数据报的运输层协议，无连接指UDP协议传输数据包之前不需要进行连接，只需要知道要传输的目的地就会将应用层传输过来的数据包进行简单的封装后直接发出，也因此UDP可以实现一对一、一对多、多对一、多对多的通信

#### tcp与udp的区别

- tcp与udp都是运输层传输协议，不同的是tcp提供的是面向连接的、可靠的基于字节流的传输协议，而udp提供的是无连接的、尽最大努力交付的基于数据报的运输层协议，tcp面向连接指的是在开始传输数据前，发送方需要与接收方进行一个三次握手的过程，并且在断开连接前要进行一个四次挥手的过程，这个过程也是在为可靠性服务，同时交换序列号，也因为这个原因，tcp只能1对1传输，而udp无连接是指它在发送报文前只需要知道接收方的ip就可以发送，也因此它就可以做到一个1对n传输，tcp的可靠性基于三次握手四次挥手，应答机制与重传机制来达到这个目的，而udp尽最大努力交付

#### HTTP

- HTTP翻译过来就是超文本传输协议，超文本指传输的内容不止是文本，还包括图片、视频等内容，传输指这个协议是将内容从一个终端传输到另一个终端，而协议就是说它是一个默认，终端双方都是通过一个默认的方式来进行网络传输

#### HTTPS

- https其实就是在http协议的通信接口部分使用了SSL协议和TLS协议作为替代；原先http存在的问题是明文传输导致通信可能被窃听，不验证通信方的身份导致通信方可能被伪装，以及不验证报文的完整性可能导致报文被篡改；https分别使用加密解决明文传输问题，用数字证书来确定通信方的身份，使用MD5或SHA-1等散列值校验算法完成完整性检验；SSL使用非对称加密与对称加密混合加密的方式，即对对称加密的密匙使用非对称加密，对主体使用对称加密，这样接收方需要先使用更加安全的非对称加密取得对称加密的密匙，再用这个密匙对主体部分解密；其原因是非对称加密的效率更低，对称加密的效率更高，非对称加密适用于少量重要数据的传输使用数字证书保证非对称加密的公匙的正确性。服务端会给客户端发送非对称密匙的公匙，自己保存私匙，保证除了服务端没有能够取得使用这个公匙加密的数据；客户端在收到公匙后，使用权威的数字证书进行认证，确认这个公匙是服务端发送的之后，再用这个公匙对对称加密的密匙进行加密，这样，攻击者就无法获取到对称加密的密匙了；使用数字签名判断证书的安全，目前最广泛的数字签名算法是SHA-RSA算法使用散列算法对数据进行完整性校验，使用较多的是基于MD5或SHA的MAC算法保证消息的完整性

#### 应用层通信

- 应用层的通行协议有http、https、DNS、FTP，应用层使用Socket进行通信

### 跨域问题

- **跨域**，指的是浏览器不允许当前网站的脚本访问非同源的其他网站，是否同源根据网站的ip地址，协议与端口决定，只有这三者完全相同才可以进行访问。它是由浏览器的同源策略造成的，是浏览器施加的安全限制。浏览器会从两个方面实现同源策略：针对接口的请求和针对Dom的查询；解决跨域问题的方式有跨域资源共享（CORS），JSONP

#### DNS域名解析

1. 客户端发起DNS递归查询，检查浏览器缓存中是否缓存过该域名对应的IP地址，如果在浏览器缓存中没有找到IP，那么将继续查找本机系统是否缓存过IP
3. 向本地域名解析服务系统发起域名解析的请求
4. 向根域名解析服务器发起域名解析请求
5. 根域名服务器返回gTLD域名解析服务器地址
6. 向gTLD服务器发起解析请求
7. gTLD服务器接收请求并返回Name Server服务器
8. Name Server服务器返回IP地址给本地服务器
9. 本地域名服务器缓存解析结果
10. 返回解析结果给用户

### 实操

#### 如何排查CPU使用率高

一般来说，cpu利用率过高可能是因为	1.程序计算比较密集	2.程序死循环	3.IO读写高

- 使用top命令找到cpu利用率较高的进程的进程id，先看统计的cpu利用率，再看表中的%cpu占用高的进程
- 再使用top命令根据进程id找到对应的线程id  top -Hp PID
- 然后使用java提供的jstack工具打印进程快照并重定向到文件中，将上面取到的线程id转化为16进制比如pritnf，然后在快照中找到对应的线程内容

#### 如何优化sql

- 如果不能确定是哪条sql执行比较慢的话，可以对线程进行分析，或者使用慢查询日志分析出执行时间比较慢的的sql，然后explain这条sql的执行计划，查看它有没有使用索引，甚至可以单独执行这条语句，使用show innodb status\G，查看执行sql语句前后的缓存的情况，然后对sql语句进行优化，例如避免查询*，避免索引失效，给join字段加索引，或者对库表结构进行优化

#### 如何排查线程安全性问题

#### jstack的使用

- jstack可以生成当前时刻指定进程中线程的快照信息，可以帮助定位程序问题，如长时间停顿、cpu占用率高等问题

### 项目

- 本项目由我们工作室成员和其它工作室成员共同完成的，我们工作室的完成后台，有其它工作室的成员完成前台也就是用户页面；

#### 项目介绍

##### 简单介绍

- 我们做的是一个商城项目，像我们后台的话有用户登录，用户管理，分类管理，商品管理，购物车模块、订单管理等模块

##### 项目背景，比如项目需求怎么来的

- 项目是由老师发起的，说这种系统型项目还是比较常见的，而且与我们之前的自己练手的学生管理系统、图书管理系统会有相似的地方，这个项目主要是让我们去体验一个团队开发的流程，以及在团队开发中会遇到的问题，与团队成员的交流问题，代码的规范问题，面向接口的好处

##### 项目开发周期，人数，开发工具

项目使用了两个月左右完成，从4月初到6月初完成，像我们后台系统的话总共有4个人完成项目，项目过程中使用的jdk8、idea、spring5.8、mysql5.7、redis6.2，然后由于对于git并没有那么熟，然后我们指导老师就说我们也是个小项目，在公司一定会用git这样的版本管理工具的，你们可以暂时先不使用，顺便了解一下不使用这些工具的缺点

##### 项目有多少张表，你负责哪些表，有哪些字段，字段间的关系，如何进行表的设计或者说表的优化

- 项目主要有用户表（用户ID，密码）、用户信息表（用户信息id，用户名。。。）、地址信息表（地址信息id。。。 ）、分类表、商品表、订单表等，还有一些用于关联表之间关系的表，比如映射分类和商品，商品与分类的关系可以是n：1或者是n：n，我们还是决定将这个映射关系单独出来，可以达到一个n：n的关系，也就是一个商品可以属于多个分类；设计表的时候，先进行需求分析，写出概念结构，然后根据概念结构设计ER图，并分析属性冲突、命名冲突，然后是根据ER图设计数据库表

##### 关于线程安全方面的解决

#### 项目开发流程

- 需求分析，分析有哪些模块是必须的，比如用户管理模块对应用户信息表的管理，分类模块对应分类表的管理

- 任务分配，就是协调一下自己想做哪些模块
- 数据库表的设计，要与相关的人员进行交流

#### 我负责的块

- 用户表的设计，用户表分表为用户表和用户信息表，方便从多个渠道进行登录与关联，同时进行账号登录的时候就不需要读取全部的用户数据
- 分类表的设计，使用了parentId字段
- 使用了pageHelper进行分页，pageHelper里面有几个比较重要的成员变量，

#### 项目难点

- 首先先说我在技术上碰到的难点，主要是交流上面的难点，就是接口文档写的不够规范，第一个，就是跨域问题，当时我还不知道浏览器有这个同源策略，一直以为是接口问题，就是以为是访问不到这个接口，然后排查接口路径